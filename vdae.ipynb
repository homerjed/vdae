{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "from jaxtyping import Key, Array\n",
    "import optax\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "from sklearn import datasets\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from models import ResidualNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, mlp_kwargs, *, key):\n",
    "        self.net = eqx.nn.MLP(**mlp_kwargs, key=key)\n",
    "    \n",
    "    def __call__(self, x_t, t):\n",
    "        # This needs to output mu(x_t, t), sigma(x_t, t) for variational distribution\n",
    "        # return self.net(jnp.concatenate([x_t.flatten(), t])) \n",
    "        # This is actually score of log(q(z|x_t, t))\n",
    "\n",
    "        mu_t, sigma_t = jnp.split(self.net(jnp.concatenate([x_t.flatten(), t])), 2)\n",
    "        return jax.scipy.stats.multivariate_normal.logpdf(z, mu_t, jnp.diag(sigma_t))\n",
    "\n",
    "    def encode(self, x_t, t):\n",
    "        return self.(x_t, t)\n",
    "\n",
    "    def score(self, z, x_t, t):\n",
    "        return jax.jacfwd(self, argnums=1)(z, x_t, t)\n",
    "\n",
    "    def prior_log_prob_z(self, z):\n",
    "        return jax.scipy.stats.norm.logpdf(z).sum()\n",
    "\n",
    "    def kl(self, x_t, t):\n",
    "        mu_t, sigma_t = jnp.split(self(x_t, t), 2) \n",
    "        return mu.T @ mu + sigma.sum() - mu.size - jnp.prod(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDAE(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    score_network: ResidualNetwork\n",
    "\n",
    "    def __init__(self, encoder, score_network):\n",
    "        self.encoder = encoder\n",
    "        self.score_network = score_network\n",
    "\n",
    "    def score(self, x_t, t):\n",
    "        return self.score_network(x_t, t) + self.encoder.score(x_t, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.key(0)\n",
    "\n",
    "score_network = ResidualNetwork(\n",
    "    in_size=2, \n",
    "    out_size=2, \n",
    "    width_size=128, \n",
    "    depth=2, \n",
    "    y_dim=1, # Just scalar time\n",
    "    activation=jax.nn.gelu, \n",
    "    key=key\n",
    ")\n",
    "\n",
    "encoder = Encoder(\n",
    "    dict(\n",
    "        in_size=2 + 1, # [x_t, t]\n",
    "        out_size=1 + 1, # Latent dim = 1\n",
    "        width_size=32, \n",
    "        depth=2, \n",
    "        activation=jax.nn.tanh\n",
    "    ),\n",
    "    key=key\n",
    ")\n",
    "\n",
    "score_network = eqx.tree_deserialise_leaves(\"sgm.eqx\", score_network)\n",
    "\n",
    "vdae = VDAE(encoder, score_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mu, sigma):\n",
    "    # Assuming sigma is diagonal elements of covariance\n",
    "    return mu.T @ mu + sigma.sum() - mu.size - jnp.prod(sigma)\n",
    "\n",
    "def multivariate_normal(mu, sigma):\n",
    "    return tfd.MultivariateNormalFullCovariance(mu, jnp.diag(sigma))\n",
    "\n",
    "def multivariate_prior(dim):\n",
    "    return tfd.MultivariateNormalFullCovariance(jnp.zeros(dim), jnp.eye(dim))\n",
    "\n",
    "# multivariate_normal(jnp.zeros(10), jnp.eye(10)).kl_divergence(multivariate_prior(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "int_beta = lambda t: t  # Try experimenting with other options here!\n",
    "\n",
    "weight = lambda t: 1 - jnp.exp(-int_beta(t))  # Just chosen to upweight the region near t=0.\n",
    "\n",
    "\n",
    "def dataloader(x, batch_size, *, key):\n",
    "    dataset_size = x.shape[0]\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        key, subkey = jr.split(key, 2)\n",
    "        perm = jr.permutation(subkey, indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield x[batch_perm]\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def single_loss_fn(encoder, score_network, weight, int_beta, x, t, key):\n",
    "    # Encoder training objective given trained diffusion model\n",
    "    t = jnp.atleast_1d(t)\n",
    "\n",
    "    # Diffusion loss calculations\n",
    "    mean = x * jnp.exp(-0.5 * int_beta(t))\n",
    "    var = jnp.maximum(1. - jnp.exp(-int_beta(t)), 1e-5)\n",
    "    std = jnp.sqrt(var)\n",
    "    noise = jr.normal(key, x.shape)\n",
    "    x_t = mean + std * noise\n",
    "\n",
    "    mu_t, sigma_t = jnp.split(encoder(x_t, t), 2) \n",
    "\n",
    "    score = score_network(x_t, t) + encoder.score(x_t, t)\n",
    "\n",
    "    # Score of encoder model plus score of diffusion model\n",
    "    return weight(t) * jnp.square(score + noise / std) - kl_loss(mu_t, sigma_t)\n",
    "\n",
    "\n",
    "def batch_loss_fn(encoder, score_network, weight, int_beta, x, t1, key):\n",
    "    batch_size = x.shape[0]\n",
    "    tkey, losskey = jr.split(key)\n",
    "    losskey = jr.split(losskey, batch_size)\n",
    "    # Low-discrepancy sampling over t to reduce variance\n",
    "    t = jr.uniform(tkey, (batch_size,), minval=0., maxval=t1 / batch_size)\n",
    "    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n",
    "    loss_fn = partial(single_loss_fn, encoder, score_network, weight, int_beta)\n",
    "    loss_fn = jax.vmap(loss_fn)\n",
    "    return jnp.mean(loss_fn(x, t, losskey))\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(encoder, score_network, weight, int_beta, x, t1, key, opt_state, opt_update):\n",
    "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
    "    loss, grads = loss_fn(encoder, score_network, weight, int_beta, x, t1, key)\n",
    "    updates, opt_state = opt_update(grads, opt_state, encoder)\n",
    "    encoder = eqx.apply_updates(encoder, updates)\n",
    "    key = jr.split(key, 1)[0]\n",
    "    return loss, encoder, key, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.adamw(1e-3)\n",
    "\n",
    "opt_state = opt.init(eqx.filter(encoder, eqx.is_array)) # Gradients only needed for encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = datasets.make_moons(10_000, noise=0.05)\n",
    "X, Y = jnp.asarray(X), jnp.asarray(Y)[:, jnp.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ff2ce25d6d4199ba4b86b461f33bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Encoder.__call__() missing 1 required positional argument: 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trange(num_steps) \u001b[38;5;28;01mas\u001b[39;00m bar:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     15\u001b[0m         bar, dataloader(X, batch_size, key\u001b[38;5;241m=\u001b[39mloader_key)\n\u001b[1;32m     16\u001b[0m     ):\n\u001b[0;32m---> 17\u001b[0m         value, encoder, train_key, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscore_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mint_beta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         total_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m         total_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m, in \u001b[0;36mmake_step\u001b[0;34m(encoder, score_network, weight, int_beta, x, t1, key, opt_state, opt_update)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_step\u001b[39m(encoder, score_network, weight, int_beta, x, t1, key, opt_state, opt_update):\n\u001b[1;32m     56\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mfilter_value_and_grad(batch_loss_fn)\n\u001b[0;32m---> 57\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_beta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m opt_update(grads, opt_state, encoder)\n\u001b[1;32m     59\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(encoder, updates)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36mbatch_loss_fn\u001b[0;34m(encoder, score_network, weight, int_beta, x, t1, key)\u001b[0m\n\u001b[1;32m     49\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m partial(single_loss_fn, encoder, score_network, weight, int_beta)\n\u001b[1;32m     50\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(loss_fn)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosskey\u001b[49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36msingle_loss_fn\u001b[0;34m(encoder, score_network, weight, int_beta, x, t, key)\u001b[0m\n\u001b[1;32m     31\u001b[0m noise \u001b[38;5;241m=\u001b[39m jr\u001b[38;5;241m.\u001b[39mnormal(key, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     32\u001b[0m x_t \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m+\u001b[39m std \u001b[38;5;241m*\u001b[39m noise\n\u001b[0;32m---> 34\u001b[0m mu_t, sigma_t \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msplit(\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m     36\u001b[0m score \u001b[38;5;241m=\u001b[39m score_network(x_t, t) \u001b[38;5;241m+\u001b[39m encoder\u001b[38;5;241m.\u001b[39mscore(x_t, t)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Score of encoder model plus score of diffusion model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoder.__call__() missing 1 required positional argument: 't'"
     ]
    }
   ],
   "source": [
    "key, train_key, loader_key = jr.split(key, 3) \n",
    "\n",
    "t1 = 1.\n",
    "batch_size = 1000 \n",
    "lr = 1e-3\n",
    "num_steps = 100_000\n",
    "\n",
    "int_beta = lambda t: t  # Try experimenting with other options here!\n",
    "weight = lambda t: 1 - jnp.exp(-int_beta(t))  # Just chosen to upweight the region near t=0.\n",
    "\n",
    "total_value = 0\n",
    "total_size = 0\n",
    "with trange(num_steps) as bar:\n",
    "    for step, data in zip(\n",
    "        bar, dataloader(X, batch_size, key=loader_key)\n",
    "    ):\n",
    "        value, encoder, train_key, opt_state = make_step(\n",
    "            encoder, \n",
    "            score_network, \n",
    "            weight, \n",
    "            int_beta, \n",
    "            data, \n",
    "            t1, \n",
    "            train_key, \n",
    "            opt_state, \n",
    "            opt.update\n",
    "        )\n",
    "        total_value += value.item()\n",
    "        total_size += 1\n",
    "        if (step % 100) == 0 or step == num_steps - 1:\n",
    "            bar.set_postfix_str(f\"Loss={total_value / total_size:.3E}\")\n",
    "            total_value = 0\n",
    "            total_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
