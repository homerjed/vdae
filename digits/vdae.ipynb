{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "import diffrax as dfx\n",
    "from jaxtyping import Key, Array\n",
    "import optax\n",
    "import einops\n",
    "from sklearn import datasets\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import ResidualNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, mlp_kwargs, *, key):\n",
    "        self.net = eqx.nn.MLP(**mlp_kwargs, key=key)\n",
    "\n",
    "    def __call__(self, x_t, t):\n",
    "        return self.encode(x_t, t)\n",
    "    \n",
    "    def variational_posterior_log_prob(self, key, x_0, x_t, t):\n",
    "        # This is the score of log(q(z|x_t, t))\n",
    "        mu_t, sigma_t = self.encode(x_t, t)\n",
    "        # Encode input data\n",
    "        mu_0, sigma_0 = self.encode(x_0, 0.) # z ~ q(z|x_0, 0), as per diagram\n",
    "        z = jr.multivariate_normal(key, mu_0, jnp.diag(sigma_0))\n",
    "        return jax.scipy.stats.multivariate_normal.logpdf(z, mu_t, jnp.diag(sigma_t))\n",
    "\n",
    "    def encode(self, x_t, t):\n",
    "        t = jnp.atleast_1d(t)\n",
    "        x_t_t = jnp.concatenate([x_t.flatten(), t])\n",
    "        mu_t, log_sigma_t = jnp.split(self.net(x_t_t), 2)\n",
    "        return mu_t, jnp.exp(log_sigma_t)\n",
    "\n",
    "    def score(self, key, x, x_t, t): \n",
    "        # Score of variational distribution q(z|x_t, t) defined by encoder\n",
    "        return jax.jacfwd(self.variational_posterior_log_prob, argnums=2)(key, x, x_t, t)\n",
    "\n",
    "    def prior_log_prob_z(self, z):\n",
    "        return jax.scipy.stats.norm.logpdf(z).sum() # Unit Gaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mu, sigma):\n",
    "    # Assuming sigma is diagonal elements of covariance (NOTE: sigma exp'd)\n",
    "    return mu.T @ mu + sigma.sum() - mu.size - jnp.prod(sigma)\n",
    "\n",
    "\n",
    "def int_beta(t):\n",
    "    return t\n",
    "\n",
    "\n",
    "def weight(t): \n",
    "    return 1. - jnp.exp(-int_beta(t))  \n",
    "\n",
    "\n",
    "def dataloader(x, batch_size, *, key):\n",
    "    dataset_size = x.shape[0]\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        key, subkey = jr.split(key, 2)\n",
    "        perm = jr.permutation(subkey, indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield x[batch_perm]\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def single_loss_fn(encoder, score_network, weight, int_beta, x, t, key):\n",
    "    # Encoder training objective given trained diffusion model\n",
    "    key_score, key_noise = jr.split(key)\n",
    "    t = jnp.atleast_1d(t)\n",
    "\n",
    "    # Diffusion loss calculations\n",
    "    mean = x * jnp.exp(-0.5 * int_beta(t))\n",
    "    var = jnp.maximum(1. - jnp.exp(-int_beta(t)), 1e-5)\n",
    "    std = jnp.sqrt(var)\n",
    "    noise = jr.normal(key_noise, x.shape)\n",
    "    x_t = mean + std * noise\n",
    "\n",
    "    mu_t, sigma_t = encoder.encode(x_t, t)\n",
    "\n",
    "    _score = score_network(x_t, t) \n",
    "    _score_encoder = encoder.score(key_score, x, x_t, t) # d_(x_t)[log q(z|x_t, t)]\n",
    "    score = _score + _score_encoder # Bayes rule for scores\n",
    "\n",
    "    # Score of encoder model plus score of diffusion model\n",
    "    return weight(t) * jnp.square(score + noise / std) - kl_loss(mu_t, sigma_t)\n",
    "\n",
    "\n",
    "def batch_loss_fn(encoder, score_network, weight, int_beta, x, t1, key):\n",
    "    batch_size = x.shape[0]\n",
    "    key_t, key_L = jr.split(key)\n",
    "    keys = jr.split(key_L, batch_size)\n",
    "\n",
    "    # Low-discrepancy sampling over t to reduce variance\n",
    "    t = jr.uniform(key_t, (batch_size,), minval=0., maxval=t1 / batch_size)\n",
    "    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n",
    "\n",
    "    loss_fn = partial(single_loss_fn, encoder, score_network, weight, int_beta)\n",
    "    return jnp.mean(jax.vmap(loss_fn)(x, t, keys))\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(encoder, score_network, weight, int_beta, x, t1, key, opt_state, opt_update):\n",
    "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
    "    loss, grads = loss_fn(encoder, score_network, weight, int_beta, x, t1, key)\n",
    "    updates, opt_state = opt_update(grads, opt_state, encoder)\n",
    "    encoder = eqx.apply_updates(encoder, updates)\n",
    "    key = jr.split(key, 1)[0]\n",
    "    return loss, encoder, key, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_digits()\n",
    "X, Y = data[\"data\"], data[\"target\"]\n",
    "X, Y = jnp.asarray(X), jnp.asarray(Y)[:, jnp.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.key(0)\n",
    "\n",
    "_, data_dim = X.shape\n",
    "latent_dim = 8\n",
    "embed_dim = 16\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(eqx.Module):\n",
    "    emb: jax.Array\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        half_dim = dim // 2\n",
    "        emb = jnp.log(10000.) / (half_dim - 1)\n",
    "        self.emb = jnp.exp(jnp.arange(half_dim) * -emb)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        emb = x * jax.lax.stop_gradient(self.emb) \n",
    "        emb = jnp.concatenate((jnp.sin(emb), jnp.cos(emb)), axis=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "score_network = ResidualNetwork(\n",
    "    in_size=data_dim, \n",
    "    out_size=data_dim, \n",
    "    width_size=512, \n",
    "    depth=2, \n",
    "    y_dim=embed_dim, # Just scalar time\n",
    "    activation=jax.nn.gelu, \n",
    "    time_embedder=SinusoidalPosEmb(embed_dim),\n",
    "    key=key\n",
    ")\n",
    "\n",
    "score_network = eqx.tree_deserialise_leaves(\"sgm.eqx\", score_network)\n",
    "\n",
    "encoder = Encoder(\n",
    "    dict(\n",
    "        in_size=data_dim + 1, # [x_t, t]\n",
    "        out_size=2 * latent_dim, # Latent dim = 1, outputing mu, sigma\n",
    "        width_size=128, \n",
    "        depth=2, \n",
    "        activation=jax.nn.tanh\n",
    "    ),\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce871e2f814c41ed829a087cd56df513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key, train_key, loader_key = jr.split(key, 3) \n",
    "\n",
    "t1 = 1.\n",
    "batch_size = 1000 \n",
    "lr = 1e-4\n",
    "num_steps = 200\n",
    "\n",
    "opt = optax.adamw(1e-3)\n",
    "opt_state = opt.init(eqx.filter(encoder, eqx.is_array)) # Gradients only needed for encoder\n",
    "\n",
    "losses = []\n",
    "with trange(num_steps) as bar:\n",
    "    for step, data in zip(bar, dataloader(X, batch_size, key=loader_key)):\n",
    "\n",
    "        value, encoder, train_key, opt_state = make_step(\n",
    "            encoder, \n",
    "            score_network, \n",
    "            weight, \n",
    "            int_beta, \n",
    "            data, \n",
    "            t1, \n",
    "            train_key, \n",
    "            opt_state, \n",
    "            opt.update\n",
    "        )\n",
    "\n",
    "        losses.append(value)\n",
    "        bar.set_postfix_str(f\"Loss={value:.3E}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(jax.jacfwd, argnums=2)\n",
    "def q_z_x_t_t(encoder, z, x_t, t):\n",
    "    mu_t, sigma_t = encoder.encode(x_t, t)\n",
    "    return jax.scipy.stats.multivariate_normal.logpdf(z, mu_t, jnp.diag(sigma_t))\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def single_sample_fn(\n",
    "    score_network, encoder, int_beta, data_shape, latent_shape, dt0, t1, key\n",
    "):\n",
    "\n",
    "    def drift(t, y, args):\n",
    "        (z, encoder) = args\n",
    "        _, beta = jax.jvp(int_beta, (t,), (jnp.ones_like(t),))\n",
    "        # Score of variational posterior\n",
    "        score_q = jax.jacfwd(q_z_x_t_t, argnums=2)(encoder, z, y, t)\n",
    "        score = score_network(y, t)\n",
    "        return -0.5 * beta * (y + (score + score_q)) \n",
    "\n",
    "    key_y1, key_z = jr.split(key)\n",
    "    term = dfx.ODETerm(drift)\n",
    "    solver = dfx.Tsit5()\n",
    "    t0 = 0.\n",
    "    y1 = jr.normal(key_y1, data_shape)\n",
    "    z = jr.normal(key_z, latent_shape) # Sample latents like a VAE\n",
    "    sol = dfx.diffeqsolve(term, solver, t1, t0, -dt0, y1, args=(z, encoder)) # Reverse time, solve from t1 to t0\n",
    "    return sol.ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jed.Homer/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/core.py:678: FutureWarning: unhashable type: <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>. Attempting to hash a tracer will lead to an error in a future JAX release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE4ElEQVR4nO3VMQHAMAzAsKz8OWefKbSHhMCfv93dAYCZObcDAHiHKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBAfu8DBwYENNNsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_shape = (data_dim,)\n",
    "latent_shape = (latent_dim,)\n",
    "dt0 = 0.01\n",
    "sample_size = 1\n",
    "\n",
    "key, sample_key = jr.split(key)\n",
    "sample_keys = jr.split(sample_key, sample_size ** 2)\n",
    "sample_fn = partial(\n",
    "    single_sample_fn, \n",
    "    score_network, \n",
    "    encoder, \n",
    "    int_beta, \n",
    "    data_shape, \n",
    "    latent_shape, \n",
    "    dt0, \n",
    "    t1\n",
    ")\n",
    "sample = jax.vmap(sample_fn)(sample_keys)\n",
    "\n",
    "sample = einops.rearrange(\n",
    "    sample, \n",
    "    \"(n1 n2) (h w) -> (n1 h) (n2 w)\", \n",
    "    n1=sample_size, \n",
    "    n2=sample_size, \n",
    "    h=8, \n",
    "    w=8\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(sample, cmap=\"gray_r\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(1797,), (1797, 8)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/util.py:311\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trace_context_in_key:\n\u001b[0;32m--> 311\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/util.py:304\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 304\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/lax/lax.py:156\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 156\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/lax/lax.py:172\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1797,), (1797, 8)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mu, sigma \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(encoder\u001b[38;5;241m.\u001b[39mencode, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))(X, \u001b[38;5;241m0.\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m Z \u001b[38;5;241m=\u001b[39m mu \u001b[38;5;241m+\u001b[39m \u001b[43mjr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(Z\u001b[38;5;241m.\u001b[39mflatten(), bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:265\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    263\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:101\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2, \u001b[38;5;241m/\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/numpy/util.py:381\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    379\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    380\u001b[0m check_for_prngkeys(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/numpy/util.py:250\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    249\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 250\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/phd/random/vit_basic/vit/lib/python3.12/site-packages/jax/_src/lax/lax.py:172\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1797,), (1797, 8)]"
     ]
    }
   ],
   "source": [
    "mu, sigma = jax.vmap(encoder.encode, in_axes=(0, None))(X, 0.)\n",
    "Z = mu + jr.normal(key, (len(X),)) * sigma\n",
    "\n",
    "plt.hist(Z.flatten(), bins=64, density=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
